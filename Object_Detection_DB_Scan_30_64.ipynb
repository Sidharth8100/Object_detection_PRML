{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow as tf  # For CIFAR-10 loading\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and preprocess CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)  # Flatten\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=100)  # Retain more variance\n",
        "train_pca = pca.fit_transform(x_train_scaled)\n",
        "test_pca = pca.transform(x_test_scaled)\n",
        "train_labels = y_train\n",
        "test_labels = y_test\n",
        "\n",
        "# Print explained variance to verify PCA\n",
        "print(f\"Explained Variance Ratio (cumulative): {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "# Step 1: Apply DBSCAN on training data\n",
        "def apply_dbscan(train_pca, eps=20, min_samples=7):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    cluster_labels = dbscan.fit_predict(train_pca)\n",
        "    return cluster_labels\n",
        "\n",
        "train_cluster_labels = apply_dbscan(train_pca)\n",
        "\n",
        "# Step 2: Map clusters to actual class labels\n",
        "def map_clusters_to_labels(cluster_labels, true_labels):\n",
        "    cluster_mapping = {}\n",
        "    unique_clusters = set(cluster_labels) - {-1}  # Ignore noise points\n",
        "\n",
        "    for cluster in unique_clusters:\n",
        "        indices = np.where(cluster_labels == cluster)[0]\n",
        "        cluster_labels = true_labels[indices]\n",
        "\n",
        "        if len(cluster_labels) > 0:\n",
        "            most_common_label = np.bincount(cluster_labels).argmax()\n",
        "            cluster_mapping[cluster] = most_common_label\n",
        "\n",
        "    return cluster_mapping\n",
        "\n",
        "cluster_mapping = map_clusters_to_labels(train_cluster_labels, train_labels)\n",
        "\n",
        "# Step 3: Train kNN classifier and predict test labels\n",
        "def predict_with_knn(train_pca, train_labels, test_pca, k=10):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(train_pca, train_labels)\n",
        "    return knn.predict(test_pca)\n",
        "\n",
        "test_cluster_labels = predict_with_knn(train_pca, train_labels, test_pca)\n",
        "\n",
        "# Map test cluster labels to actual class labels\n",
        "test_predictions = np.array([cluster_mapping.get(label, label) for label in test_cluster_labels])\n",
        "\n",
        "# Calculate accuracy\n",
        "def compute_accuracy(true_labels, predicted_labels):\n",
        "    return accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "accuracy = compute_accuracy(test_labels, test_predictions)\n",
        "print(\"Predicted Test Labels:\", test_predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp9NN1Xz9e_x",
        "outputId": "120c6382-c123-415c-9ca0-f31a50322f9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Explained Variance Ratio (cumulative): 0.8983\n",
            "Predicted Test Labels: [2 8 8 ... 5 6 4]\n",
            "Accuracy: 30.64%\n"
          ]
        }
      ]
    }
  ]
}